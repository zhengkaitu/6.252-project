{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "partial-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "peripheral-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection function definition\n",
    "\n",
    "def projection_l1_builder(l: float = 1.0):\n",
    "    def projection_l1(w: np.ndarray) -> np.ndarray:\n",
    "        proj_w = w\n",
    "\n",
    "        if np.linalg.norm(w, ord=1) > l:\n",
    "            # solve the piecewise-linear equation by sorting\n",
    "            sorted_abs_w = np.sort(np.abs(w))[::-1]\n",
    "            for k in range(len(sorted_abs_w)):\n",
    "                if (np.sum(sorted_abs_w[:k+1]) - l) / (k+1) >= sorted_abs_w[k]:\n",
    "                    break\n",
    "            else:\n",
    "                k += 1\n",
    "\n",
    "            tau = np.mean(sorted_abs_w[:k]) - l / k\n",
    "\n",
    "            assert abs(np.sum(max(abs(wi) - tau, 0) for wi in w) / l - 1) < 1e-6, \\\n",
    "                f\"w: {w}, \" \\\n",
    "                f\"sum: {np.sum(max(abs(wi) - tau, 0) for wi in w)}, \" \\\n",
    "                f\"tau: {tau}\"\n",
    "            \n",
    "            # soft thresholding\n",
    "            for i in range(len(proj_w)):\n",
    "                if proj_w[i] > tau:\n",
    "                    proj_w[i] -= tau\n",
    "                elif proj_w[i] < -tau:\n",
    "                    proj_w[i] += tau\n",
    "                else:\n",
    "                    proj_w[i] = 0\n",
    "\n",
    "            assert np.linalg.norm(proj_w, ord=1) / l - 1 < 1e-6, np.linalg.norm(proj_w, ord=1)\n",
    "\n",
    "        return proj_w\n",
    "\n",
    "    return projection_l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "necessary-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective and jacobian definition\n",
    "\n",
    "def regression_builder():\n",
    "    \"\"\"\n",
    "        data: np.ndarray, (n, d)\n",
    "        labels: np.ndarray, (n,)\n",
    "    \"\"\"\n",
    "    def regression_f(w, data, labels):\n",
    "        res = np.dot(data, w) - labels              # (n,)\n",
    "        f = 0.5 * np.linalg.norm(res)\n",
    "        \n",
    "        n_sample_feval = len(data)\n",
    "        f_flop_counts = None\n",
    "\n",
    "        return f, n_sample_feval, f_flop_counts\n",
    "\n",
    "    def regression_J(w, data, labels):\n",
    "        res = np.dot(data, w) - labels              # (n,)\n",
    "        J = np.dot(res, data)\n",
    "        \n",
    "        n_sample_gradients = len(data)\n",
    "        J_flop_counts = None\n",
    "\n",
    "        return J, n_sample_gradients, J_flop_counts\n",
    "\n",
    "    return regression_f, regression_J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "functioning-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer definition: pgd\n",
    "\n",
    "def pgd(data: np.ndarray, labels: np.ndarray, w0,\n",
    "        f, J, proj,\n",
    "        lr, w_tol, f_tol, max_epochs) -> Tuple[List[np.ndarray], int]:\n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    \n",
    "    # initialization\n",
    "    ws = [proj(w0)]\n",
    "    f0, n_sample_feval, f_flop_counts = f(ws[-1], data, labels)\n",
    "    fs = [f0]\n",
    "    n_total_gradients = 0\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for _ in tqdm(range(max_epochs)):\n",
    "        w0 = ws[-1]\n",
    "        grad, n_sample_gradients, J_flop_counts = J(w0, data, labels)\n",
    "        \n",
    "        w1 = w0 - lr * grad          # gradient descent\n",
    "        w1 = proj(w1)                # projection\n",
    "        ws.append(w1)\n",
    "        \n",
    "        n_total_gradients += n_sample_gradients\n",
    "\n",
    "        f0 = fs[-1]\n",
    "        f1, n_sample_feval, f_flop_counts = f(w1, data, labels)\n",
    "        fs.append(f1)\n",
    "        \n",
    "        # stopping criteria, w-norm\n",
    "        if np.linalg.norm(w1 - w0, ord=1) / np.linalg.norm(w0, ord=1) < w_tol:\n",
    "            break\n",
    "        \n",
    "        # stopping criteria, f-norm\n",
    "        if abs(f0 / f1 - 1) < f_tol:\n",
    "            break        \n",
    "\n",
    "    return ws, n_total_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "magnetic-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer definition: sgd\n",
    "\n",
    "def sgd(data: np.ndarray, labels: np.ndarray, w0,\n",
    "        f, J, proj,\n",
    "        lr, w_tol, f_tol, max_epochs,\n",
    "        batch_size=1, max_stall_count=50) -> Tuple[List[np.ndarray], int]:\n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    \n",
    "    # initialization\n",
    "    ws = [proj(w0)]\n",
    "    f0, n_sample_feval, f_flop_counts = f(ws[-1], data, labels)\n",
    "    fs = [f0]\n",
    "    n_total_gradients = 0\n",
    "    w_stall_counter = 0\n",
    "    f_stall_counter = 0\n",
    "    \n",
    "    for _ in tqdm(range(max_epochs)):\n",
    "        # shuffle index (in place)\n",
    "        indices = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # stochastic gradient descent loop\n",
    "        for i in indices:        \n",
    "            w0 = ws[-1]\n",
    "            grad, n_sample_gradients, J_flop_counts = J(w0, data[i:i+1], labels[i:i+1])\n",
    "\n",
    "            w1 = w0 - lr * grad          # gradient descent\n",
    "            w1 = proj(w1)                # projection\n",
    "            ws.append(w1)\n",
    "\n",
    "            n_total_gradients += n_sample_gradients\n",
    "\n",
    "            f0 = fs[-1]\n",
    "            f1, n_sample_feval, f_flop_counts = f(w1, data, labels)\n",
    "            fs.append(f1)\n",
    "\n",
    "            # stopping criteria, w-norm\n",
    "            if np.linalg.norm(w1 - w0, ord=1) / np.linalg.norm(w0, ord=1) < w_tol:\n",
    "                if w_stall_counter < max_stall_count:\n",
    "                    w_stall_counter += 1\n",
    "                else:\n",
    "                    print(f\"max stall count {max_stall_count} reached for w norm\")\n",
    "                    break\n",
    "            else:\n",
    "                w_stall_counter = 0\n",
    "\n",
    "            # stopping criteria, f-norm\n",
    "            if abs(f0 / f1 - 1) < f_tol:\n",
    "                if f_stall_counter < max_stall_count:\n",
    "                    f_stall_counter += 1\n",
    "                else:\n",
    "                    print(f\"max stall count {max_stall_count} reached for f norm\")\n",
    "                    break        \n",
    "            else:\n",
    "                f_stall_counter = 0\n",
    "            \n",
    "        else:\n",
    "            # continue if the inner loop wasn't broken.\n",
    "            continue\n",
    "        \n",
    "        # inner loop was broken, break the outer.\n",
    "        break\n",
    "\n",
    "    return ws, n_total_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "prepared-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer definition: sag\n",
    "\n",
    "def sag(data: np.ndarray, labels: np.ndarray, w0,\n",
    "        f, J, proj,\n",
    "        lr, w_tol, f_tol, max_epochs,\n",
    "        batch_size=1, max_stall_count=50) -> Tuple[List[np.ndarray], int]:\n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    \n",
    "    # initialization\n",
    "    ws = [proj(w0)]\n",
    "    f0, n_sample_feval, f_flop_counts = f(ws[-1], data, labels)\n",
    "    fs = [f0]\n",
    "    n_total_gradients = 0\n",
    "    \n",
    "    grads = np.zeros_like(data, dtype=np.float32)     # TODO: fix initialization\n",
    "    avg_grad = np.zeros_like(w0, dtype=np.float32)\n",
    "    \n",
    "    w_stall_counter = 0\n",
    "    f_stall_counter = 0\n",
    "    \n",
    "    for _ in tqdm(range(max_epochs)):\n",
    "        # shuffle index (in place)\n",
    "        indices = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in indices:        \n",
    "            w0 = ws[-1]\n",
    "            prev_grad = grads[i]\n",
    "            new_grad, n_sample_gradients, J_flop_counts = J(w0, data[i:i+1], labels[i:i+1])\n",
    "\n",
    "            avg_grad = avg_grad + (new_grad - prev_grad) # / data.shape[0]\n",
    "            w1 = w0 - lr * avg_grad      # gradient descent\n",
    "            w1 = proj(w1)                # projection\n",
    "            ws.append(w1)\n",
    "            \n",
    "            grads[i] = new_grad\n",
    "            n_total_gradients += n_sample_gradients\n",
    "\n",
    "            f0 = fs[-1]\n",
    "            f1, n_sample_feval, f_flop_counts = f(w1, data, labels)\n",
    "            fs.append(f1)\n",
    "\n",
    "            # stopping criteria, w-norm\n",
    "            if np.linalg.norm(w1 - w0, ord=1) / np.linalg.norm(w0, ord=1) < w_tol:\n",
    "                if w_stall_counter < max_stall_count:\n",
    "                    w_stall_counter += 1\n",
    "                else:\n",
    "                    print(f\"max stall count {max_stall_count} reached for w norm\")\n",
    "                    break\n",
    "            else:\n",
    "                w_stall_counter = 0\n",
    "\n",
    "            # stopping criteria, f-norm\n",
    "            if abs(f0 / f1 - 1) < f_tol:\n",
    "                if f_stall_counter < max_stall_count:\n",
    "                    f_stall_counter += 1\n",
    "                else:\n",
    "                    print(f\"max stall count {max_stall_count} reached for f norm\")\n",
    "                    break        \n",
    "            else:\n",
    "                f_stall_counter = 0\n",
    "            \n",
    "        else:\n",
    "            # Continue if the inner loop wasn't broken.\n",
    "            continue\n",
    "        \n",
    "        # Inner loop was broken, break the outer.\n",
    "        break\n",
    "\n",
    "    return ws, n_total_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "electronic-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem definition\n",
    "\n",
    "f, J = regression_builder()\n",
    "proj = projection_l1_builder(l=1.0)\n",
    "\n",
    "n = 10000\n",
    "d = 20\n",
    "\n",
    "w = np.random.random(d)\n",
    "w = w / np.linalg.norm(w, ord=1)\n",
    "\n",
    "data = np.random.random((n, d))\n",
    "labels = np.dot(data, w)\n",
    "\n",
    "w0 = np.random.random(d)\n",
    "w_tol = 1e-6\n",
    "f_tol = 1e-6\n",
    "max_epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "durable-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nos/miniconda3/envs/projects/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  1%|â–         | 13/1000 [00:00<00:01, 939.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [0.05767626 0.06759437 0.07666225 0.08149776 0.0239234  0.06432924\n",
      " 0.00539999 0.05808347 0.00731672 0.09864687 0.03519868 0.04257602\n",
      " 0.02149977 0.0174237  0.08543414 0.07448893 0.01353679 0.0706693\n",
      " 0.01343755 0.08460479]\n",
      "\n",
      "total sample gradient evaluations: 140000\n",
      "\n",
      "last 5 ws:\n",
      "[0.05767597 0.06759306 0.07666188 0.08149737 0.02392447 0.06432851\n",
      " 0.00540002 0.05808348 0.0073164  0.09864657 0.0351989  0.04257633\n",
      " 0.02150022 0.01742344 0.08543416 0.07448923 0.01353757 0.07067007\n",
      " 0.01343691 0.08460543]\n",
      "[0.05767603 0.06759396 0.07666202 0.08149755 0.02392337 0.06432894\n",
      " 0.00539984 0.05808324 0.00731651 0.09864666 0.03519856 0.04257589\n",
      " 0.02149967 0.01742352 0.08543396 0.07448877 0.01353675 0.07066924\n",
      " 0.01343726 0.08460474]\n",
      "[0.05767623 0.06759421 0.0766622  0.08149771 0.02392353 0.06432915\n",
      " 0.00539999 0.05808347 0.00731668 0.09864683 0.03519871 0.04257606\n",
      " 0.02149983 0.01742366 0.08543415 0.07448897 0.01353689 0.0706694\n",
      " 0.01343747 0.08460487]\n",
      "[0.05767624 0.06759432 0.07666222 0.08149773 0.0239234  0.0643292\n",
      " 0.00539997 0.05808344 0.00731669 0.09864684 0.03519866 0.04257601\n",
      " 0.02149976 0.01742367 0.08543412 0.07448891 0.01353679 0.07066929\n",
      " 0.01343751 0.08460478]\n",
      "[0.05767626 0.06759435 0.07666224 0.08149775 0.02392342 0.06432923\n",
      " 0.00539999 0.05808347 0.00731671 0.09864686 0.03519868 0.04257603\n",
      " 0.02149978 0.01742369 0.08543414 0.07448894 0.0135368  0.07066931\n",
      " 0.01343754 0.0846048 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main: pgd\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "ws, n_total_gradients = pgd(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    w0=w0,\n",
    "    f=f,\n",
    "    J=J,\n",
    "    proj=proj,\n",
    "    lr=lr,\n",
    "    w_tol=w_tol,\n",
    "    f_tol=f_tol,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "print(f\"w:\\n {w}\\n\")\n",
    "print(f\"total sample gradient evaluations: {n_total_gradients}\\n\")\n",
    "print(f\"last 5 ws:\")\n",
    "[print(ws[i]) for i in range(len(ws)-5, len(ws))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "suburban-crazy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/nos/miniconda3/envs/projects/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  0%|          | 1/1000 [00:01<20:34,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max stall count 50 reached for w norm\n",
      "w:\n",
      " [0.05767626 0.06759437 0.07666225 0.08149776 0.0239234  0.06432924\n",
      " 0.00539999 0.05808347 0.00731672 0.09864687 0.03519868 0.04257602\n",
      " 0.02149977 0.0174237  0.08543414 0.07448893 0.01353679 0.0706693\n",
      " 0.01343755 0.08460479]\n",
      "\n",
      "total sample gradient evaluations: 12113\n",
      "\n",
      "last 5 ws:\n",
      "[0.05767402 0.06759142 0.07666545 0.08149237 0.02392226 0.06432745\n",
      " 0.00540788 0.05808295 0.00731528 0.09864568 0.03520349 0.04257458\n",
      " 0.02150242 0.0174254  0.0854301  0.07448419 0.01354606 0.07067179\n",
      " 0.01343477 0.08460247]\n",
      "[0.05767402 0.06759142 0.07666544 0.08149237 0.02392226 0.06432744\n",
      " 0.00540788 0.05808295 0.00731528 0.09864568 0.03520349 0.04257458\n",
      " 0.02150242 0.0174254  0.0854301  0.07448419 0.01354606 0.07067179\n",
      " 0.01343477 0.08460247]\n",
      "[0.05767402 0.06759142 0.07666544 0.08149237 0.02392226 0.06432744\n",
      " 0.00540788 0.05808295 0.00731528 0.09864568 0.03520349 0.04257458\n",
      " 0.02150242 0.0174254  0.0854301  0.07448419 0.01354606 0.07067179\n",
      " 0.01343477 0.08460247]\n",
      "[0.05767403 0.06759143 0.07666544 0.08149236 0.02392227 0.06432744\n",
      " 0.00540788 0.05808295 0.00731528 0.09864567 0.03520347 0.04257459\n",
      " 0.02150243 0.01742541 0.08543009 0.07448419 0.01354605 0.0706718\n",
      " 0.01343477 0.08460246]\n",
      "[0.05767403 0.06759142 0.07666544 0.08149237 0.02392227 0.06432743\n",
      " 0.00540788 0.05808295 0.00731528 0.09864567 0.03520348 0.04257459\n",
      " 0.02150243 0.01742541 0.08543009 0.07448418 0.01354605 0.07067181\n",
      " 0.01343477 0.08460247]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main: sgd\n",
    "\n",
    "lr = 1e-2\n",
    "\n",
    "ws, n_total_gradients = sgd(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    w0=w0,\n",
    "    f=f,\n",
    "    J=J,\n",
    "    proj=proj,\n",
    "    lr=lr,\n",
    "    w_tol=w_tol,\n",
    "    f_tol=f_tol,\n",
    "    max_epochs=max_epochs,\n",
    "    batch_size=1,\n",
    "    max_stall_count=50\n",
    ")\n",
    "\n",
    "print(f\"w:\\n {w}\\n\")\n",
    "print(f\"total sample gradient evaluations: {n_total_gradients}\\n\")\n",
    "print(f\"last 5 ws:\")\n",
    "[print(ws[i]) for i in range(len(ws)-5, len(ws))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "understood-indication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlr = 1e-1\\n\\nws, n_total_gradients = sag(\\n    data=data,\\n    labels=labels,\\n    w0=w0,\\n    f=f,\\n    J=J,\\n    proj=proj,\\n    lr=lr,\\n    w_tol=w_tol,\\n    f_tol=f_tol,\\n    max_epochs=max_epochs,\\n    batch_size=1,\\n    max_stall_count=50\\n)\\n\\nprint(f\"w:\\n {w}\\n\")\\nprint(f\"total sample gradient evaluations: {n_total_gradients}\\n\")\\nprint(f\"last 5 ws:\")\\n[print(ws[i]) for i in range(len(ws)-5, len(ws))]\\n'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main: sag -- in progress; will fix if needed\n",
    "\n",
    "\"\"\"\n",
    "lr = 1e-1\n",
    "\n",
    "ws, n_total_gradients = sag(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    w0=w0,\n",
    "    f=f,\n",
    "    J=J,\n",
    "    proj=proj,\n",
    "    lr=lr,\n",
    "    w_tol=w_tol,\n",
    "    f_tol=f_tol,\n",
    "    max_epochs=max_epochs,\n",
    "    batch_size=1,\n",
    "    max_stall_count=50\n",
    ")\n",
    "\n",
    "print(f\"w:\\n {w}\\n\")\n",
    "print(f\"total sample gradient evaluations: {n_total_gradients}\\n\")\n",
    "print(f\"last 5 ws:\")\n",
    "[print(ws[i]) for i in range(len(ws)-5, len(ws))]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-genetics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
